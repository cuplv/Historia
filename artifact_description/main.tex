\documentclass{acmart} %\settopmatter{printfolios=true,printccs=false,printacmref=false} % full template

%%%%%% optimizations for build speed and review



%%%%%%
%%To turn off red line numbers, use documentclass without "review"
%\documentclass[acmsmall,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

%%% Packages
\usepackage[nosubfig,nographicx,nonatbib,noxcolor,nohyperref,notheoremenvs]{bec}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n} %supress asmmath font warning
\setboolean{showjedi}{false}
\setboolean{showcomments}{true} % turn off todo comments too
\usepackage{tikz} 
\usetikzlibrary{automata, positioning,arrows,arrows.meta,fit}
\usepackage{multirow}
\usepackage{setspace} % for "\setstretch" macro used by specbox
\usepackage{empheq}
\usepackage[clock]{ifsym}

% booleans to turn off individual sections
\newcommand{\inputsection}[1]{\ifthenelse{\boolean{#1}}{\input{#1}}{\TODO{====== temp exclusion section #1 for build =======}}}



%%
%% \BibTeX command to typeset BibTeX logo in the docs
%\AtBeginDocument{%
%  \providecommand\BibTeX{{%
%    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%%
%% These commands are for a JOURNAL article.
%\renewcommand{\reftxt}[2]{\hyperref[#2]{#1~\ref*{#2}}}
%\renewcommand{\lineref}[1]{\reftxt{line}{line:#1}} 
%\newcommand{\refprog}[1]{\reftxt{location}{#1}}
%\newcommand{\refhistimp}[1]{\reftxt{History Implication}{#1}}

% hyperref options
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\bibliographystyle{ACM-Reference-Format}
%\citestyle{acmauthoryear}

%%%%double spacing for hand review
%%% also comment out 3200 through 3204 in acmart.cls (enforces no messing with line spacing)
%\usepackage{setspace}
%\doublespacing % put after document
%%%%%%%%%


\begin{document}

\input{macros}
%\newcommand{\plusmini}{\scalebox{0.8}{$+$}}
%\newcommand{\minusmini}{\scalebox{0.8}{$-$}}
%\newcommand{\expos}{\ensuremath{\SetName{Ex}^{\plusmini}}}
%\newcommand{\exneg}{\ensuremath{\SetName{Ex}^{\minusmini}}}
\newcommand{\newls}{CBCFTL\xspace}
%\newcommand{\ignore}[1]{}
%
\title{Artifact - Historia: Refuting Callback Reachability with Message-History Logics}

\begin{abstract}
\end{abstract}


\maketitle
%\input{generated/specMacros}
\section{Introduction}
This document explains the artifact for the Historia paper \cite{conf/oopsla/Meier23}.
The goal of this document is to first give a set of instructions for reproducing the experimental results and then give a technical explanation of how the implementation connects to the technical contributions.
We have automated the steps to reproduce the results with scripts explained below.

When using \toolname on its own, the input is a compiled Android application in the form of an APK file, a \newls specification defined in a JSON format, and an assertion at a location defined by a JSON data structure.
Outputs are either ``safe'' or ``alarm'' in which case an abstract message history witnessing the alarm will be available.

\section{Prerequisites - Running the Historia Docker Container}

We have configured the experiments to be run within two Docker containers provided with this artifact.
These Docker files may be found in the root directory of this archive and is labeled \codej{historia.docker}.  Please follow the instructions to install docker from \url{https://docs.docker.com/engine/install/}.

Importing the docker containers can be done with the following command.

\begin{lstlisting}[language=bash]
  docker import historia.docker
  docker import experimentdb.docker
\end{lstlisting}
\TODO{double check import works after container creation}

In order to start the Docker containers, run the following command in the root directory.  This will start two Docker containers.

%docker run --memory="8G" --memory-swap="8G" --rm -it historia bash
\begin{lstlisting}[language=bash]
 docker compose -f docker-compose.yml up
\end{lstlisting}

All subsequent steps may be done through the web interface at \url{http://localhost:8888}.  Opening this URL should show a Jupyter notebook as shown by Figure \ref{fig:jupyter}.  


\begin{figure}[H]
    \includegraphics*[scale=0.35]{jupyter.png}
    \caption{Your web browser should look like this if the Docker images started correctly.}
    \label{fig:jupyter}
\end{figure}

When finished, the following commands will close down the containers and remove them.

\begin{lstlisting}[language=bash]
    docker compose down
    docker compose rm
\end{lstlisting}
% ctrl-c will close down the docker containers and ``\codej{docker compose rm}'' will clean up the images.

\paragraph{System Requirements} 
We have split the instructions so that a subset of the experiments may be run on a reasonable laptop and included full instructions if access to a server is available.
The instructions labeled "minimal resources" may be run on a reasonably modern laptop that has 8GB of ram in addition to ram used by other processes. 

Instructions labeled with "full resources" can be run on more powerful systems.  We used multiple servers with an AMD EPYC 7763 64-Core Processor and 256GB of ram.  Usage of these resources was granted to our project by Chameleon Cloud \cite{keahey2020lessons}.


\section{Reproducing the Historia Results}

Each subsection here corresponds to a table in the evaluation section.  We have labeled the subsections first with the research question (\req{1} or \req{2}), the table number (Table 1, Table 2, Table 3), and finally, the resource requirements as described earlier (minimal resources or full resources).

\subsection{\req{1} - Table 1 - minimal resources}

To run the experiments for the first table, open the terminal in the Jupyter notebook listed earlier and run the following command:
\begin{lstlisting}[language=bash]
    ./runExperiments.sh
\end{lstlisting}

The experiments in Table 1 are run as a Scala unit tests defined in the file: 

\codej{implementation/src/test/scala/edu/colorado/plv/bounder/symbolicexecutor/Experiments.scala}.  

Each benchmark is labeled by \codej{test([description])} where [description] has a row number and short english description.
The source code for each benchmark is a string stored in a variable named \codej{src}.
This source code is compiled into the APK automatically when running the unit test.

The set of \newls specifications for each row are defined by row in the \codej{ExperimentSpecs} object within \codej{Experiments.scala}.  These specifications may be found in \codej{src/main/scala/edu/colorado/plv/bounder/lifestate/Specification.scala}.

The output will be printed to the screen in a log format.  Each row of Table 1 may be found labeled by ``Row'' followed by the row number and version (i.e. ``bug'' or ``fix'').  For example, the buggy version of \apGa is ``Row 1 bug'' and the fixed version is ``Row 1 fix''.  

\newcommand{\cTimeout}{\showclock{0}{45}}
\newsavebox{\cSafeBox}
\savebox{\cSafeBox}{
\hspace{-.15cm}\scalebox{0.7}{
\tikz[baseline=-0.7ex]{
\node (a) [circle,draw=black] {
};
\node (b) [draw=none] {\scalebox{1}{\checkmark}};
}}\hspace{-.1cm}
}
\newcommand{\cSafe}{
\usebox{\cSafeBox}
}

\newsavebox{\cSafeToFiveBox}
\savebox{\cSafeToFiveBox}{
\hspace{-.15cm}\scalebox{0.7}{
\tikz[baseline=-0.7ex]{
\node (a) [circle,draw=black] {
};
\node (b) [draw=none] {\scalebox{1}{5}};
}}\hspace{-.1cm}
}
\newcommand{\cSafeToFive}{
\usebox{\cSafeToFiveBox}
}

\newsavebox{\cAlarmBox}
\savebox{\cAlarmBox}{
\hspace{-.1cm}\scalebox{0.7}{
\tikz[baseline=-0.7ex]{
\node (a) [circle,draw=black] {
};
\node (b) [draw=none] {\scalebox{1}{!}};
}}\hspace{-.0cm}
}
\newcommand{\cAlarm}{\usebox{\cAlarmBox}}
%

Below is an explanation of each column in the table:
\begin{enumerate}
    \item Pattern cb,ret - The number of callbacks and returns in the bug pattern: Integer labeled ``cbSize''.
    \item Pattern ci - the number of callins in the bug pattern: Integer labeled ``syntCi''.
    \item Historia specs - number of specs written for the benchmark.  Integer labeled ``spec count''. The specific specs may be found in \codej{Experiments.scala} and \codej{Specification.scala} as described earlier.
    \item Historia cb - number of callbacks that may be matched by the \newls spec: Integer labeled ``matchedCb''.
    \item Historia cbret - number of callback returns that may be matched by the \newls spec: Integer labeled ``matchedCbRet''.
    \item Historia ci - number of callins matched by the spec: Integer labeled ``matchedSyntCi''.
    \item Historia time - runtime: 
    \item Historia res - The result of the analysis: There will be several rows of text displaying the data from the table.  The result of verification labeled with ``actual:'' and may say ``Witnessed'' (\cAlarm), ``Timeout'' (\cTimeout), or ``Proven'' (\cSafe).  For reference, it also prints the expected result after ``expected:''.  If the actual and expected results differ, the unit test fails.
\end{enumerate}


% \TODO{SM: explain how to compare flowdroid and infer}

\subsection{\req{2} - Table 2 and Table 3 - Viewing results}

% We suggest first viewing the output of our run before attempting to run the \toolname experiments from scratch.
Since the full experiments take a long time to run, start by viewing our results in the Jupyter interface.
During the experiments, jobs and results are stored in a database in the \codej{historia_postgres} docker container started by \codej{docker compose}.
To load the pre-computed results, open a terminal in the Jupyter notebook and run the following script to load the final database:

\begin{lstlisting}[language=bash]
    ./loadPreRunTable2.sh
\end{lstlisting}

The results can be viewed by opening the notebook \codej{ShowTable2and3.ipynb} shown on the left.  After opening, select ``Kernel'' -> ``Restart Kernel and Run All Cells''.
This notebook reads the results from the database and displays the Historia results for Table 2 and Table 3 (Shown under heading titles ``Table 1'' and ``Table 2''). Additional statistics about the run may be viewed in this notebook as well. Statistics about the overall runtime may be found toward the end of the notebook \codej{MonitorTable2Run.ipynb}.

Sizes for the call graphs were computed by setting a breakpoint after call graph generation in both Flowdroid and \toolname.  The call graph was then filtered for methods under the package declared by the application's manifest.  We recorded these call graph sizes in the file \codej{callgraphCount.csv}.

\subsection{\req{2} - Table 2 - minimal resources}

\newcommand{\numsubset}{10}
The full set of 1090 locations can take an extremely long time to run (over 15 days of CPU time).  Therefore, we recommend running a subset.


The notebook \codej{RunTable2-Minimal-Resources.ipynb} will run by clearing the database and uploading the locations in the ``dataSubset'' directory.  These locations will be processed by the Historia worker.  
The directory ``dataSubset'' contains \numsubset~locations that don't use too much ram and don't time out.  Locations are stored as ``.json'' files.
Running should take around 20 minutes depending on the system.

Progress can be tracked through the \codej{MonitorTable2Run.ipynb} notebook.  
Each time a cell in this notebook is run using ``shift-enter'', the output will be updated.
% All cells in this notebook may be run repeatedly with ``shift-enter'' to track the status and outputs.
When the \codej{completed\_jobs} row under the ``Track Total Jobs'' heading reaches \numsubset, the subset of locations is completed running.

Once completed, the last cell of the notebook \codej{ShowTable2and3.ipynb} can compare the results just generated against our results.  It will print out the path to the config file, the result from the most recent run, and finally, the result from the run used for the paper.  Please note that memory errors often appear as the process simply being killed and generally do not show an error message.

\paragraph{Changing the subset} The subset of apps may be changed to a random selection by deleting the contents of the ``dataSubset'' directory and re-running the last cell of \codej{ShowTable2and3.ipynb}.  Please note that many locations will not complete on the 8 GB of ram configured by the docker compose file (see the later instructions to remove this restriction).  


\subsection{\req{2} - Table 3 - minimal resources}

The rows in Table 3 may be run using the \codej{ShowTable2and3.ipynb} notebook.
Under the heading ``Table 3 Data - Write Specifications For Individual Samples'' each cell (e.g. marked by ``Specify\_0'') corresponds to one row in Table 3.

The output of the cell will contain the name of the app in the row. The configuration file used as input, and the directory where results are written.  For example:

\begin{lstlisting}
    home/notebooks/SpecGen/com.darshancomputing.BatteryIndicatorPro/SensitiveDerefCallinCaused/0/config.json
    App Name: BatteryBot
    out directory: /home/notebooks/SpecGen/com.darshancomputing.BatteryIndicatorPro/SensitiveDerefCallinCaused/0
    apk path: /fdroid/com.darshancomputing.BatteryIndicatorPro/12.0.0/apk/com.darshancomputing.BatteryIndicatorPro\_26016.apk
\end{lstlisting}

Since we set the timeout to one hour, we recommend only running the rows that are shown to complete in the table.  These rows can be run by uncommenting the call to \codej{runHistoriaWithSpec()} at the bottom of the cell.  For example, the // can be removed from the second two lines of BatteryBot:

\begin{lstlisting}
    //Uncomment the following two lines to run Historia on this sample from Table 3
    // val result = Specify_1.runHistoriaWithSpec()
    // println(s"Result Summary: ...")
\end{lstlisting}

Making it look like: 

\begin{lstlisting}
    //Uncomment the following two lines to run Historia on this sample from Table 3
    val result = Specify_1.runHistoriaWithSpec()
    println(s"Result Summary: ...")
\end{lstlisting}


Afterwards, running the cell will take slightly longer than before  and produce the ``res'' column of Table 3.  For example, the output may look like:

\begin{lstlisting} 
    Result Summary: Proven
\end{lstlisting}


\TODO{notebook and description for table 3 rerun}


\subsection{\req{1} - Table 1 - full resources}

In order to run the full version of Table 1, a system with at least 120G of ram must be used. To run, restart the Docker container using the following command:

\begin{lstlisting}[language=bash]
  docker compose -f docker-compose-full-resources.yml up
\end{lstlisting}

Then, repeat the instructions from the ``minimal resources'' section.  This should take about an hour to run on a comparable system.

If this completes successfully, the output should be the same as earlier but will include ``Row 4 fix''.  

\subsection{\req{2} - Table 2 - full resources}

In order to run the full version of Table 2, we strongly suggest using multiple servers.
You can use the \codej{MonitorTable2Run.ipynb} notebook to estimate the expected remaining runtime after a few jobs have completed (See the ``Estimate Time Until Completion of All Jobs'' cell).

The Docker compose file \codej{docker-compose-full-resources.yml} shows how to start and connect worker containers.  Scaling to multiple systems can be done with Docker swarm \url{https://docs.docker.com/engine/swarm/} or simple SSH tunnels connecting port 5432 from the workers to the Postgres database container. Once the workers and server are configured, the full experiments may be run through the \codej{RunTable2-Full-Resources.ipynb} notebook.  To add more workers on a single system, the \codej{--scale worker=n} flag may be added to the docker command.

The results may be viewed the same as was explained in the ``minimal resources'' section earlier.


\subsection{\req{2} - Table 3 - full resources}

The instructions for Table 3 may also be run under the full resources version of the docker containers, but the results are similar.

\iffalse
\section{Running and Interpreting \toolname on Custom Inputs}
\TODO{second priority}

Historia may be run on arbitrary Android applications as long as an APK can be compiled in debug mode.  This is usually accomplished using the command \codej{./gradlew assembleDebug} but will vary from application to application. A location and safety property must be chosen ahead of time.  However, we recommend only writing \newls specifications as needed.

\paragraph{Running \toolname Through Jupyter}
The recommended way to run \toolname is using a Jupyter notebook.  This allows the inputs to the tool to be defined using the Scala data structures for input rather than JSON.

\TODO{give sample app to walk through this process}

\TODO{explain this process completely}

\TODO{explain counter examples}


% \section{Debugging issues}

% \subsection{Docker desktop}
% \textbf{Problem:} The tests exit prematurely and no output is printed to the screen.
% \textbf{Solution:} Check that docker desktop is configured with enough ram.  In the docker desktop dashboard, under settings->resources, ensure the memory slider is set to at least 8GB.

\section{Developing for \toolname}
\TODO{fourth priority}

In this section, we explain the implementation of each technical contribution in \toolname.

\subsection{Running Unit Tests}

\TODO{this may need to go in later connecting formalism section}


\subsection{Application-Only Control-Flow Graph}

\TODO{}

\subsection{Message-History Program Logic (MHPL)}

\TODO{}

\subsection{Callback Control-Flow Temporal Logic (CBCFTL)}

\TODO{}

\subsection{Combining Abstract Message Histories with Callback Control Flow}

\TODO{}

\subsection{Optimizations}

\TODO{}

\fi
%======================


\bibliography{conference.short,bec.short}% main.short,

%\ifTR
%\clearpage
%\appendix
%\inputsection{appendix}
%\fi

\end{document}

%%%%old sections
%\input{decision-procedure}
